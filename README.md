# ğŸ¯ AI-Powered Interview Prep System

> Fully local AI â€” No API keys, No cloud costs, No rate limits.
>
> **Stack:** Ollama (Mistral LLM) â†’ Flask AI Service â†’ Spring Boot Backend

---

## ğŸ—ï¸ Project Structure

```
ohiooo/
â”œâ”€â”€ ai-service/                  â† DAY 1: Flask + Ollama
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ requirements.txt
â”‚
â””â”€â”€ interview-service/           â† DAY 2: Spring Boot
    â”œâ”€â”€ pom.xml
    â””â”€â”€ src/main/
        â”œâ”€â”€ java/com/interviewprep/interviewservice/
        â”‚   â”œâ”€â”€ InterviewServiceApplication.java
        â”‚   â”œâ”€â”€ config/
        â”‚   â”‚   â””â”€â”€ AppConfig.java
        â”‚   â”œâ”€â”€ controller/
        â”‚   â”‚   â””â”€â”€ InterviewController.java
        â”‚   â”œâ”€â”€ dto/
        â”‚   â”‚   â””â”€â”€ GenerateRequest.java
        â”‚   â”œâ”€â”€ entity/
        â”‚   â”‚   â””â”€â”€ InterviewSession.java
        â”‚   â”œâ”€â”€ enums/
        â”‚   â”‚   â”œâ”€â”€ RoleType.java
        â”‚   â”‚   â””â”€â”€ QuestionType.java
        â”‚   â”œâ”€â”€ repository/
        â”‚   â”‚   â””â”€â”€ InterviewSessionRepository.java
        â”‚   â””â”€â”€ service/
        â”‚       â””â”€â”€ AIClientService.java
        â””â”€â”€ resources/
            â””â”€â”€ application.properties
```

---

## ğŸš€ How to Run (Full Stack)

### Step 1 â€“ Start Ollama
```bash
ollama serve
# (In another terminal) pull the model if not done yet:
ollama pull mistral
```

### Step 2 â€“ Start Flask AI Service
```bash
cd ai-service
pip install -r requirements.txt
python app.py
# Runs on http://localhost:5000
```

### Step 3 â€“ Start Spring Boot
```bash
cd interview-service
mvn spring-boot:run
# Runs on http://localhost:8080
```

---

## ğŸ§ª API Endpoints

### Flask AI Service (port 5000)
| Method | Endpoint | Description |
|--------|----------|-------------|
| GET  | `/health` | Health check |
| POST | `/generate-subjective` | Generate 5 subjective questions |
| POST | `/generate-mcq` | Generate 5 MCQ questions |

### Spring Boot (port 8080)
| Method | Endpoint | Description |
|--------|----------|-------------|
| GET  | `/api/interview/health` | Health check |
| POST | `/api/interview/generate-subjective` | Proxied through Flask â†’ Ollama |
| POST | `/api/interview/generate-mcq` | Proxied through Flask â†’ Ollama |

### Sample Request Body
```json
{
  "role": "Java Developer",
  "level": "medium"
}
```

---

## ğŸ­ Supported Roles

| Role | Subjective | MCQ | Coding |
|------|-----------|-----|--------|
| Java Developer | âœ… | âœ… | âœ… |
| Python Developer | âœ… | âœ… | âœ… |
| C Programmer | âœ… | âœ… | âœ… |
| C++ Programmer | âœ… | âœ… | âœ… |
| DevOps Engineer | âœ… | âœ… | âŒ |
| QA Engineer | âœ… | âœ… | âŒ |
| Data Analyst | âœ… | âœ… | âŒ |
| Web Developer | âœ… | âœ… | âŒ |

---

## ğŸ” System Flow

```
User Request
    â†“
Spring Boot :8080  (/api/interview/generate-subjective)
    â†“
Flask AI Service :5000  (/generate-subjective)
    â†“
Ollama :11434  (mistral model)
    â†“
JSON Questions returned up the chain
```

---

## ğŸ—„ï¸ H2 Database Console

While Spring Boot is running, access the in-memory DB at:
```
http://localhost:8080/h2-console
JDBC URL: jdbc:h2:mem:interviewdb
Username: sa
Password: (empty)
```
